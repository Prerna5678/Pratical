{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b5dd29bd"
      },
      "source": [
        "# Task\n",
        "Develop a rule-based chatbot that uses a small dataset of question-answer pairs, applies basic NLP techniques like tokenization, stemming, and keyword extraction, and interacts with users through a command-line interface. Conclude by summarizing its capabilities and demonstrating its functionality with example interactions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6abb9d7a"
      },
      "source": [
        "## Data Collection\n",
        "\n",
        "### Subtask:\n",
        "Create a small dataset of question-answer pairs relevant to a specific domain for the chatbot to use. This dataset will serve as the knowledge base for the rule-based system.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "99bd0759"
      },
      "source": [
        "**Reasoning**:\n",
        "To begin creating the dataset, I will define a small set of question-answer pairs in the domain of general knowledge, as instructed. This will be represented as a Python list of dictionaries.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "56433990",
        "outputId": "8c1892fb-8467-439e-c1fd-866d133c5f8b"
      },
      "source": [
        "qa_dataset = [\n",
        "    {\"question\": \"What is the capital of France?\", \"answer\": \"The capital of France is Paris.\"},\n",
        "    {\"question\": \"Who painted the Mona Lisa?\", \"answer\": \"Leonardo da Vinci painted the Mona Lisa.\"},\n",
        "    {\"question\": \"What is the highest mountain in the world?\", \"answer\": \"Mount Everest is the highest mountain in the world.\"},\n",
        "    {\"question\": \"What is the chemical symbol for water?\", \"answer\": \"The chemical symbol for water is H2O.\"},\n",
        "    {\"question\": \"How many planets are in our solar system?\", \"answer\": \"There are eight planets in our solar system.\"}\n",
        "]\n",
        "\n",
        "print(\"Question-Answer dataset created successfully.\")\n",
        "print(f\"Number of Q&A pairs: {len(qa_dataset)}\")\n",
        "print(\"First Q&A pair:\", qa_dataset[0])"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question-Answer dataset created successfully.\n",
            "Number of Q&A pairs: 5\n",
            "First Q&A pair: {'question': 'What is the capital of France?', 'answer': 'The capital of France is Paris.'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9fb13f43"
      },
      "source": [
        "## Implement NLP Techniques\n",
        "\n",
        "### Subtask:\n",
        "Develop functions to apply basic NLP techniques such as tokenization, stemming, and keyword extraction to process user input.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e6af2291"
      },
      "source": [
        "**Reasoning**:\n",
        "I will import the necessary NLTK libraries, download required datasets, define functions for tokenization, stemming, and keyword extraction, and then demonstrate their usage, as specified in the instructions to implement NLP techniques for processing user input.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ce91e14",
        "outputId": "e4dcfba3-fdda-46fa-e6da-f243b4bb575c"
      },
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# 2. Download NLTK datasets if not already downloaded\n",
        "nltk.download('punkt', quiet=True)\n",
        "nltk.download('stopwords', quiet=True)\n",
        "nltk.download('punkt_tab', quiet=True) # Added to address LookupError for punkt_tab\n",
        "\n",
        "print(\"NLTK 'punkt', 'stopwords', and 'punkt_tab' datasets downloaded/checked successfully.\")\n",
        "\n",
        "# 3. Define tokenization function\n",
        "def tokenize_text(text):\n",
        "    return word_tokenize(text)\n",
        "\n",
        "# 4. Instantiate PorterStemmer\n",
        "stemmer = PorterStemmer()\n",
        "print(\"PorterStemmer instantiated.\")\n",
        "\n",
        "# 5. Define stemming function\n",
        "def stem_tokens(tokens):\n",
        "    return [stemmer.stem(token) for token in tokens]\n",
        "\n",
        "# 6. Get English stopwords\n",
        "english_stopwords = set(stopwords.words('english'))\n",
        "print(f\"Loaded {len(english_stopwords)} English stopwords.\")\n",
        "\n",
        "# 7. Define keyword extraction function\n",
        "def extract_keywords(user_input):\n",
        "    # a. Tokenize the input string\n",
        "    tokens = tokenize_text(user_input)\n",
        "\n",
        "    # b. Convert all tokens to lowercase and c. Filter out stopwords and non-alphabetic tokens\n",
        "    filtered_tokens = [word.lower() for word in tokens if word.isalpha() and word.lower() not in english_stopwords]\n",
        "\n",
        "    # d. Stem the remaining tokens\n",
        "    stemmed_keywords = stem_tokens(filtered_tokens)\n",
        "\n",
        "    # e. Return the list of unique stemmed keywords\n",
        "    return list(set(stemmed_keywords))\n",
        "\n",
        "print(\"NLP utility functions (tokenize_text, stem_tokens, extract_keywords) defined.\")\n",
        "\n",
        "# Demonstrate functionality with an example\n",
        "example_input = \"What is the capital of France? I want to know about it.\"\n",
        "tokenized_example = tokenize_text(example_input)\n",
        "stemmed_example = stem_tokens(tokenized_example)\n",
        "keywords_example = extract_keywords(example_input)\n",
        "\n",
        "print(f\"\\nOriginal input: '{example_input}'\")\n",
        "print(f\"Tokenized output: {tokenized_example}\")\n",
        "print(f\"Stemmed output: {stemmed_example}\")\n",
        "print(f\"Extracted keywords: {keywords_example}\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NLTK 'punkt', 'stopwords', and 'punkt_tab' datasets downloaded/checked successfully.\n",
            "PorterStemmer instantiated.\n",
            "Loaded 198 English stopwords.\n",
            "NLP utility functions (tokenize_text, stem_tokens, extract_keywords) defined.\n",
            "\n",
            "Original input: 'What is the capital of France? I want to know about it.'\n",
            "Tokenized output: ['What', 'is', 'the', 'capital', 'of', 'France', '?', 'I', 'want', 'to', 'know', 'about', 'it', '.']\n",
            "Stemmed output: ['what', 'is', 'the', 'capit', 'of', 'franc', '?', 'i', 'want', 'to', 'know', 'about', 'it', '.']\n",
            "Extracted keywords: ['capit', 'know', 'want', 'franc']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "da90a286"
      },
      "source": [
        "## Build Rule-Based System\n",
        "\n",
        "### Subtask:\n",
        "Implement a rule-based system that uses the extracted keywords and patterns from user queries to map them to appropriate, predefined responses from the collected dataset.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f7f4faed"
      },
      "source": [
        "**Reasoning**:\n",
        "To begin implementing the rule-based system, I need to preprocess the existing `qa_dataset` by extracting keywords for each question and storing them in a new structure, `processed_qa_dataset`, as specified in the instructions. This will involve iterating through the `qa_dataset` and using the `extract_keywords` function that was previously defined.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bec0710c",
        "outputId": "81c66045-21cc-4934-d8f7-417001bb6dbd"
      },
      "source": [
        "processed_qa_dataset = []\n",
        "for item in qa_dataset:\n",
        "    question_keywords = extract_keywords(item['question'])\n",
        "    processed_qa_dataset.append({\n",
        "        \"question\": item['question'],\n",
        "        \"answer\": item['answer'],\n",
        "        \"keywords\": question_keywords\n",
        "    })\n",
        "\n",
        "print(\"Processed Q&A dataset created successfully with keywords.\")\n",
        "print(f\"Number of processed Q&A pairs: {len(processed_qa_dataset)}\")\n",
        "print(\"First processed Q&A pair:\", processed_qa_dataset[0])"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed Q&A dataset created successfully with keywords.\n",
            "Number of processed Q&A pairs: 5\n",
            "First processed Q&A pair: {'question': 'What is the capital of France?', 'answer': 'The capital of France is Paris.', 'keywords': ['capit', 'franc']}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4217fe5b"
      },
      "source": [
        "**Reasoning**:\n",
        "Now that the `qa_dataset` has been preprocessed with keywords, the next step is to define the `find_answer` function. This function will take a user query, extract its keywords, compare them against the keywords in the `processed_qa_dataset`, calculate a match score, and return the best matching answer or a default response, as per the instructions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "131fe55d",
        "outputId": "341dacb6-c530-4583-bb48-58b5687552a9"
      },
      "source": [
        "def find_answer(user_query):\n",
        "    user_keywords = extract_keywords(user_query)\n",
        "    best_match_score = 0\n",
        "    best_answer = \"I'm sorry, I don't have an answer to that question.\"\n",
        "\n",
        "    if not user_keywords:\n",
        "        return best_answer\n",
        "\n",
        "    for item in processed_qa_dataset:\n",
        "        question_keywords = item['keywords']\n",
        "        # Calculate match score: number of common keywords\n",
        "        match_score = len(set(user_keywords).intersection(set(question_keywords)))\n",
        "\n",
        "        if match_score > best_match_score:\n",
        "            best_match_score = match_score\n",
        "            best_answer = item['answer']\n",
        "\n",
        "    # If no keywords matched (score remains 0) and the default answer is still present, ensure it's returned\n",
        "    if best_match_score == 0:\n",
        "        return \"I'm sorry, I don't have an answer to that question.\"\n",
        "    else:\n",
        "        return best_answer\n",
        "\n",
        "print(\"The 'find_answer' function has been defined.\")\n",
        "\n",
        "# Test the find_answer function with example queries\n",
        "print(\"\\n--- Testing find_answer function ---\")\n",
        "\n",
        "# Example 1: Clear match\n",
        "query1 = \"What is the capital of France?\"\n",
        "response1 = find_answer(query1)\n",
        "print(f\"User: {query1}\\nChatbot: {response1}\")\n",
        "\n",
        "# Example 2: Partial match / rephrased query\n",
        "query2 = \"Who painted Mona Lisa?\"\n",
        "response2 = find_answer(query2)\n",
        "print(f\"User: {query2}\\nChatbot: {response2}\")\n",
        "\n",
        "# Example 3: Different phrasing of an existing question\n",
        "query3 = \"Highest peak in the world?\"\n",
        "response3 = find_answer(query3)\n",
        "print(f\"User: {query3}\\nChatbot: {response3}\")\n",
        "\n",
        "# Example 4: Query with no match\n",
        "query4 = \"Tell me about artificial intelligence.\"\n",
        "response4 = find_answer(query4)\n",
        "print(f\"User: {query4}\\nChatbot: {response4}\")\n",
        "\n",
        "# Example 5: Another query with no match\n",
        "query5 = \"What is the weather like today?\"\n",
        "response5 = find_answer(query5)\n",
        "print(f\"User: {query5}\\nChatbot: {response5}\")\n",
        "\n",
        "# Example 6: Query with multiple common words but not enough matching keywords\n",
        "query6 = \"How many planets are there in our solar system right now?\"\n",
        "response6 = find_answer(query6)\n",
        "print(f\"User: {query6}\\nChatbot: {response6}\")\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The 'find_answer' function has been defined.\n",
            "\n",
            "--- Testing find_answer function ---\n",
            "User: What is the capital of France?\n",
            "Chatbot: The capital of France is Paris.\n",
            "User: Who painted Mona Lisa?\n",
            "Chatbot: Leonardo da Vinci painted the Mona Lisa.\n",
            "User: Highest peak in the world?\n",
            "Chatbot: Mount Everest is the highest mountain in the world.\n",
            "User: Tell me about artificial intelligence.\n",
            "Chatbot: I'm sorry, I don't have an answer to that question.\n",
            "User: What is the weather like today?\n",
            "Chatbot: I'm sorry, I don't have an answer to that question.\n",
            "User: How many planets are there in our solar system right now?\n",
            "Chatbot: There are eight planets in our solar system.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7d5bcd6b"
      },
      "source": [
        "## Develop User Interface\n",
        "\n",
        "### Subtask:\n",
        "Create a simple command-line interface (CLI) that allows users to interact with the chatbot by typing queries and receiving responses.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7a0208fa"
      },
      "source": [
        "#### Reasoning:\n",
        "To create the command-line interface, I will implement an infinite loop that continuously prompts the user for input. Inside this loop, I will capture the user's query, convert it to lowercase for case-insensitive processing, and check if it's an exit command ('quit', 'exit', 'bye'). If it is, a farewell message will be printed, and the loop will terminate. Otherwise, the user's query will be passed to the previously defined `find_answer()` function, and the chatbot's response will be displayed to the user."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e3cc182d"
      },
      "source": [
        "**Reasoning**:\n",
        "To create the command-line interface, I will implement an infinite loop that continuously prompts the user for input. Inside this loop, I will capture the user's query, convert it to lowercase for case-insensitive processing, and check if it's an exit command ('quit', 'exit', 'bye'). If it is, a farewell message will be printed, and the loop will terminate. Otherwise, the user's query will be passed to the previously defined `find_answer()` function, and the chatbot's response will be displayed to the user.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2c7eabc2",
        "outputId": "516cac6c-73a5-4227-ce58-bf3c7929b6fb"
      },
      "source": [
        "print(\"\\n--- Chatbot CLI Started ---\")\n",
        "print(\"Type 'quit', 'exit', or 'bye' to end the conversation.\")\n",
        "\n",
        "while True:\n",
        "    user_input = input(\"\\nYou: \")\n",
        "    user_input_lower = user_input.lower()\n",
        "\n",
        "    if user_input_lower in ['quit', 'exit', 'bye']:\n",
        "        print(\"Chatbot: Goodbye! Thanks for chatting.\")\n",
        "        break\n",
        "    else:\n",
        "        response = find_answer(user_input)\n",
        "        print(f\"Chatbot: {response}\")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Chatbot CLI Started ---\n",
            "Type 'quit', 'exit', or 'bye' to end the conversation.\n",
            "\n",
            "You: What is the capital of France?\n",
            "Chatbot: The capital of France is Paris.\n",
            "\n",
            "You: quiet\n",
            "Chatbot: I'm sorry, I don't have an answer to that question.\n",
            "\n",
            "You: quit\n",
            "Chatbot: Goodbye! Thanks for chatting.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c7b2e695"
      },
      "source": [
        "## Final Task\n",
        "\n",
        "### Subtask:\n",
        "Summarize the developed chatbot's capabilities, demonstrate its functionality with a few example interactions, and discuss potential next steps for improvement or extensions.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0a2ebdac"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "*   **Knowledge Base Creation**: A `qa_dataset` was successfully created with 5 distinct question-answer pairs covering general knowledge topics, serving as the chatbot's knowledge base. Each pair was later processed to include extracted keywords for matching.\n",
        "*   **NLP Utility Development**: Core NLP functions were implemented using NLTK:\n",
        "    *   `tokenize_text`: Breaks down text into individual words or punctuation.\n",
        "    *   `stem_tokens`: Reduces words to their root form (e.g., \"capital\" and \"capitals\" become \"capit\").\n",
        "    *   `extract_keywords`: Processes user input by tokenizing, converting to lowercase, removing stopwords and non-alphabetic characters, and stemming the remaining tokens to produce a list of unique keywords.\n",
        "*   **Rule-Based Matching System**: A `find_answer` function was developed to match user queries to the knowledge base:\n",
        "    *   It extracts keywords from the user's query and calculates a match score based on the number of common keywords with questions in the `processed_qa_dataset`.\n",
        "    *   The function returns the answer associated with the highest match score.\n",
        "    *   If no matching keywords are found (match score is 0), it provides a default \"I'm sorry, I don't have an answer to that question.\" response.\n",
        "    *   **Demonstrated Functionality**: The chatbot successfully answered direct questions like \"What is the capital of France?\" and \"Who painted Mona Lisa?\". It also handled rephrased queries such as \"Highest peak in the world?\" and \"How many planets are there in our solar system right now?\" by correctly identifying keywords and providing the relevant answers.\n",
        "*   **Command-Line Interface (CLI)**: A basic CLI was implemented, allowing interactive communication. Users can type queries, receive responses, and exit the conversation using commands like 'quit', 'exit', or 'bye'.\n",
        "\n",
        "### Insights or Next Steps\n",
        "\n",
        "*   **Expand Knowledge Base and NLP Sophistication**: To improve the chatbot's utility, the `qa_dataset` could be significantly expanded with more diverse and domain-specific information. Additionally, integrating more advanced NLP techniques such as named entity recognition, part-of-speech tagging, or even simple intent classification could allow the chatbot to understand more complex queries and provide more nuanced responses beyond simple keyword matching.\n",
        "*   **Enhance Matching Algorithm**: The current keyword matching relies solely on the count of shared stemmed keywords. Future improvements could involve weighting keywords based on importance (e.g., TF-IDF), considering the order of keywords, or implementing a more robust similarity metric (e.g., cosine similarity with word embeddings) to handle synonyms and semantic variations more effectively.\n"
      ]
    }
  ]
}